{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "from tensorflow import keras\n",
    "from matplotlib import pyplot as plt\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Categorization, Part I\n",
    "\n",
    "In the previous tutorials, we have learned how to classify image using deep neural networks with TensorFlow 2.x. However, image classification is only one of many important applications of deep learning. In this tutorial, we're going to dive into another important area where deep learning has made significant advancement in recent years: natural language processing. We are going to start with the task of text categorization, in which we assign classes or categories to text according to its content. Let's get started! \n",
    "\n",
    "For this tutorial, we're going to use the IMDB movie review dataset. This dataset is contained in the `tensorflow_datasets` collection which comes along with `tensorflow`.\n",
    "\n",
    "#### 1. Loading data from `tensorflow_datasets`\n",
    "\n",
    "The IMDB movie review dataset is a dataset for binary sentiment classification, comprised of 25,000 highly polar movie reviews for training and 25,000 for testing. There are also additional unlabeled data for use as well but we're not going to use them for this supervised learning task. Each review is associated with a label categorizing it as a positive or negative review.\n",
    "\n",
    "Since a piece of text is consisted of words, we cannot process them directly with a neural network architecture. The first step in a NLP task is usually to represent each word with an integer, and this is done with an \"encoder\" (not to be confused with the encoder as a neural network architecture). The encoder basically transforms a piece of text into an array of integers with a fixed vocabulary size (number of different integers). `tensorflow_datasets` comes with text encoders so we don't need to be worried about this process for now. There are different implementations of this encoder (with different vocabulary sizes), and we're going to use a version that has a vocabulary of about 8,000 tokens. Note that since sequence models almost always require padding due to variable input length, the token 0 is reserved for padding. First, let's see how to load this data from `tensorflow_datasets`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0518 16:50:21.322405 139834113791808 text_feature.py:61] TFDS datasets with text encoding are deprecated and will be removed in a future version. Instead, you should use the plain text version and tokenize the text using `tensorflow_text` (See: https://www.tensorflow.org/tutorials/tensorflow_text/intro#tfdata_example)\n"
     ]
    }
   ],
   "source": [
    "# The `load` method is the easiest way to load a dataset from `tensorflow_datasets`. It returns a `tf.data.Dataset` instance.\n",
    "# We specify the version of encoded data we desire by specifying the `subwords8k` path. \n",
    "# We specify the path to store this data with the `data_dir` argument.\n",
    "# The `with_info` option returns an additional object containing some information about this dataset.\n",
    "# The `as_supervised` option tells it to only return labeled data.\n",
    "\n",
    "imdb_dataset, info = tfds.load('imdb_reviews/subwords8k', data_dir='./datasets/', with_info=True, as_supervised=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at some properties of this dataset. First, let's inspect the `info` variable. For example, it tells us which encoder was used to generate the integer tokens from raw text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 8185\n"
     ]
    }
   ],
   "source": [
    "encoder = info.features['text'].encoder\n",
    "print('Vocabulary size: %d' % encoder.vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can directly use this encoder to encode a random piece of text that we pass to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[62, 9, 4, 612, 6, 1271, 7703, 7961, 3433, 7975]\n"
     ]
    }
   ],
   "source": [
    "# Use the `encode` method and pass it a string.\n",
    "print(encoder.encode('This is a piece of example raw text.'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the encoder transforms a string into a sequence of integers. Since 0 is reserved for padding, this encoding process always returns a list of positive numbers. We can also use the encoder to do the reverse process: translate a list of integers into a string of raw text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello TensorFlow.\n"
     ]
    }
   ],
   "source": [
    "# Use the `decode` method and pass it a list of integers.\n",
    "print(encoder.decode([4025, 222, 6307, 2327, 4043, 2120, 7975]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might wonder why a list of 7 tokens only represent 2 words. This is because encoding is not done in a word-by-word fashion with this encoder. If we take a closer look at which token corresponds to which word, we can get a better idea of the encoding mechanism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4025 ----> Hell\n",
      "222 ----> o \n",
      "6307 ----> Ten\n",
      "2327 ----> sor\n",
      "4043 ----> Fl\n",
      "2120 ----> ow\n",
      "7975 ----> .\n"
     ]
    }
   ],
   "source": [
    "for token in [4025, 222, 6307, 2327, 4043, 2120, 7975]:\n",
    "    print('%s ----> %s' % (token, encoder.decode([token])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the encoder is just transforming specific substrings into their corresponding tokens. This is one of the simplest ways of enconding, but is certainly not the best, as words are broken apart and might lose their original meaning. This is to some extent limited by our choice of vocabulary size, since 8,000 tokens cannot fully express the abundance of words that appear in the reviews. If we had a larger vocabulary there would be other ways to encode the texts while preserving their word boundaries and meaning as much as possible. We're not going into details here for the purpose of this tutorial.\n",
    "\n",
    "Now that we have loaded the entire dataset, let's split out training and test sets. This is already done by `tensorflow_datasets`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_examples, test_examples = imdb_dataset['train'], imdb_dataset['test']\n",
    "\n",
    "# We need to apply transformations for `tf.data.Dataset` to get batched data for training and test sets.\n",
    "# For the training set, we first need to shuffle the dataset, and then generate batches with a defined batch size.\n",
    "\n",
    "# Since each review has a different length, we need to pad the shorter sequences so that they're all of the same size. To do this,\n",
    "# we need to apply the `padded_batch` method, where you speficy the resulting shape for each dimension for each component.\n",
    "\n",
    "# For each data point, we have a sequence of variable size, so we specify `None` in order to pad to the longest sequence in\n",
    "# each batch. We also have a scalar label which we don't need to pad, so we just pass an empty list. \n",
    "train_dataset = train_examples.shuffle(buffer_size=10000).padded_batch(batch_size=64, padded_shapes=([None],[]))\n",
    "\n",
    "# For the test set, we don't need to shuffle the dataset but we still need to do padding on each batch.\n",
    "test_dataset = test_examples.padded_batch(batch_size=64, padded_shapes=([None],[]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of input batch (64, 1247)\n",
      "[[ 274  252 7748 ...    0    0    0]\n",
      " [  19 1612 5512 ...    0    0    0]\n",
      " [ 133   97  911 ...    0    0    0]\n",
      " ...\n",
      " [  62   32   25 ...    0    0    0]\n",
      " [  12 6081   36 ...    0    0    0]\n",
      " [7963   19 1720 ...    0    0    0]] \n",
      "\n",
      "Shape of input labels (64,)\n",
      "[1 1 1 0 1 1 0 0 0 1 0 1 0 0 0 0 1 0 1 1 1 0 1 1 0 1 1 0 1 1 0 0 1 0 0 1 0\n",
      " 1 1 0 1 1 1 1 0 1 0 0 0 1 0 1 1 1 0 0 1 0 1 0 1 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "# Let's take a look at one batch of training data.\n",
    "batch_input, batch_label = list(train_dataset.take(1).as_numpy_iterator())[0]\n",
    "print('Shape of input batch', batch_input.shape)\n",
    "print(batch_input, '\\n')\n",
    "\n",
    "print('Shape of input labels', batch_label.shape)\n",
    "print(batch_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that all sequences in the first batch are padded to a length of 1,404. The 0 tokens represent padding. Now that the dataset is presented in numerical form and padded, we are ready to build a neural network model to process it. For sequence data, we usually use recurrent neural networks (RNNs). However, before building RNN blocks, there is something we need to do, and that is embedding.\n",
    "\n",
    "#### 2. Building an RNN model\n",
    "\n",
    "Although the encoder has already transformed words into integers, they're still not in the best format to be interpreted by deep learning models. The integer encoding is arbitrary and does not capture relationships between words. A better idea is to use a vector of fixed length to represent each token. In addition, we would want similar tokens to be embedded close together. Of course, we cannot provide these embeddings for now, but we can define the length of these vectors and let the neural network figure out what vectors to embed for each token. This is called an embedding layer. \n",
    "\n",
    "Mathematically, an embedding layer is basically a 2D tensor of size [`vocab_size`, `embed_size`], meaning that each token in the vocabulary has an embedding of `embed_size`. When input passes through this layer, basically for each token a lookup is performed, taking out the row corresponding to that specific token. Therefore, after passing through the embedding layer, a sequence of length `seq_length` will become a 2D tensor of size [`seq_length`, `embed_size`]. The embedding layer is directly available in `tf.keras.layers`.\n",
    "\n",
    "For this task, let's use a two-layer bi-directional LSTM architecture. Since the model can be represented in a linear fashion, we're going to use `tf.keras.Sequential` to build the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    keras.layers.Embedding(encoder.vocab_size, 32), # Specify vocab size and embedding size. We use an embedding size of 32.\n",
    "    \n",
    "    # To use bi-directional layers, we need to wrap the layer inside of `tf.keras.layers.Bidirectional`.\n",
    "    # For RNNs, we can return different things from each layer. For this layer, we want the output from each cell so that they can be passed on to the next layer.\n",
    "    # To do this, we need to specify `return_sequences` to `True`, or else the layer would only return the output of the last cells (from the two directions).\n",
    "    keras.layers.Bidirectional(keras.layers.LSTM(32, return_sequences=True)),\n",
    "    \n",
    "    # The output of the previous layer is [batch_size, batch_longest_sequence_length, 64].\n",
    "    # The last dimension of 64 came from the concatenation of the output from two directions. \n",
    "    # You can also do summing or averaging by specifying the `merge_mode` argument.\n",
    "    \n",
    "    # For the second LSTM layer, we're going to take only the output from the last cells from each direction.\n",
    "    keras.layers.Bidirectional(keras.layers.LSTM(32)),\n",
    "    keras.layers.Dense(64, activation=keras.activations.relu), # Dense layer\n",
    "    keras.layers.Dropout(0.5), # Dropout layer to mitigate overfitting\n",
    "    keras.layers.Dense(1) # Output layer. For binary classification we can just output the probability of the positive class.\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 32)          261920    \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, None, 64)          16640     \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 64)                24832     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 307,617\n",
      "Trainable params: 307,617\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# We can call the `summary` method of our model to get an overview of our model architecture and parameters.\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Compiling and training the model\n",
    "\n",
    "We compile our model the usual way by calling the `compile` method and specifying the loss function, the optimizer to use and the metrics to calculate in each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True), # Since we have a single prediction for each data point, we should use `BinaryCrossentropy`.\n",
    "              optimizer=tf.keras.optimizers.Adam(1e-4), # We can pass a optimizer instance to the `optimizer` argument.\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "391/391 [==============================] - 1064s 3s/step - loss: 0.6842 - accuracy: 0.5144\n",
      "Epoch 2/10\n",
      "391/391 [==============================] - 1060s 3s/step - loss: 0.4428 - accuracy: 0.8010\n",
      "Epoch 3/10\n",
      "391/391 [==============================] - 1060s 3s/step - loss: 0.3159 - accuracy: 0.8764\n",
      "Epoch 4/10\n",
      "391/391 [==============================] - 1011s 3s/step - loss: 0.2635 - accuracy: 0.9030\n",
      "Epoch 5/10\n",
      "391/391 [==============================] - 978s 3s/step - loss: 0.2301 - accuracy: 0.9202\n",
      "Epoch 6/10\n",
      "391/391 [==============================] - 1573s 4s/step - loss: 0.2056 - accuracy: 0.9309\n",
      "Epoch 7/10\n",
      "391/391 [==============================] - 1200s 3s/step - loss: 0.1834 - accuracy: 0.9411\n",
      "Epoch 8/10\n",
      "391/391 [==============================] - 945s 2s/step - loss: 0.1695 - accuracy: 0.9470\n",
      "Epoch 9/10\n",
      "391/391 [==============================] - 933s 2s/step - loss: 0.1567 - accuracy: 0.9534\n",
      "Epoch 10/10\n",
      "391/391 [==============================] - 1045s 3s/step - loss: 0.1487 - accuracy: 0.9560\n"
     ]
    }
   ],
   "source": [
    "# Now, we can train our model by calling `fit`.\n",
    "history = model.fit(train_dataset, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x216 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize training performance\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "fig.set_figwidth(6)\n",
    "fig.set_figheight(3)\n",
    "\n",
    "ax1.plot(range(10), history.history['loss'])\n",
    "ax1.set_ylabel('Training loss')\n",
    "ax1.set_xlabel('Epoch')\n",
    "\n",
    "ax2.plot(range(10), history.history['accuracy'])\n",
    "ax2.set_ylabel('Training accuracy')\n",
    "ax2.set_xlabel('Epoch')\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that training has been pretty successful! Training accuracy reached to 95.6% after 10 epochs. Notice that training an RNN took much longer time than training a CNN, and this is just one of the difficulties of training RNN models, especially when you have long sequences. Let's see how well our model performs on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    391/Unknown - 319s 815ms/step - loss: 0.4521 - accuracy: 0.8484Test Loss: 0.452103\n",
      "Test Accuracy: 0.848360\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(test_dataset)\n",
    "\n",
    "print('Test Loss: %f' % test_loss)\n",
    "print('Test Accuracy: %f' % test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test performance is good, but much lower than training performance. This means that our model is suffering from the problem of overfitting. Nevertheless, this tutorial has demonstrated how we can deal with text data with neural networks. After we have the model, we can also categorize unseen movie reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.93577397]]\n"
     ]
    }
   ],
   "source": [
    "review_text = 'a punchy, gritty experience that will poke and grab at your emotions in surprising and meaningful ways'\n",
    "encoded_text = encoder.encode(review_text)\n",
    "encoded_text = tf.cast(encoded_text, tf.float32) # Model requires float input\n",
    "predictions = model.predict(tf.expand_dims(encoded_text, 0)) # Add a `batch` dimension of 1.\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This high prediction score means that the model categorizes this review as a positive one.\n",
    "\n",
    "In this tutorial we have learned how to do text categorization with an RNN. One thing that can be improved here is that we can apply masking to the paddings so that they would not be taken into account as real data, which may give us a model with better performance. In the next tutorial, we're first going to divert from text categorization a little bit and learn about text generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
